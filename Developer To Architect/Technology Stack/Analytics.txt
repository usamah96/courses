Technology Stack
--



Data Movement
--
	
About Logstash
..

We can think logstash as a pipe where data can be inserted on one end and received at another end
The pipe has several components which has some responsibility when the data has arrived to it

The logs can be generated by anything like system logs, application logs, database logs, etc
Lets say we have Apache Web Server which logs all the incoming requests which looks something like this
..
  "127.0.0.1 - - [05/Feb/2012 17:15:12+0000] GET / HTTP 1.1/ 200 140 - Mozilla/5.0...."
..

These logs are now inserted into the pipe to the first component either by Push mechanism or Pull mechanism
Push sourse can be Filebeat which is provided by Logstash itself
Pull source can be our database where logstash pull data from our any queue also

Depending upon the source, our first component "Input Plugin" is chosen like any File, Beats, Kafka, Redis, etc
Then the Input Plugin transfers this data to the "Queue" component where data is made available for processing. This queue mechanism can be 
in-memory or file based (for persistence). File based mechanism is more reliable because data is persisted and if logstash crashes or goes down the
data can be recovered when it comes up again.
After that, the data is transferred to "Filter Plugin" component where the data is transformed into required format. In this example the data is
a string with space separated. This data can be transformed into JSON, CSV, Grok, Dissect using their libraries
After transforming, the data is transferred to "Output Plugin" component and that plugin is related to what our destination is. If our destination
is ElasticSearch then we use ElasticSearch plugin so that ES can pull the data from Logstash easily. Similarly if we intent to put data into Kafka,
Redis, Hadoop or send via email then we can choose the output plugin accordingly. We can generate alert or also store it in S3 bucket as an archive.

Finally when the destination receives the data, it acknowledges about it and that acknowledgment is sent all the way back to the Queue component
of logstash which deletes the data from it. In this way, atleast one time delivery is guarenteed from it. Also the queue component applies
backpressure when the rate of receiving data at destination is relatively slow that that of logstash. The queue will built up very large in this
case so the queue component stops receeving data from input plugin component till old data is being sent to the destination

The queue component can also be replaced with an external queue for better reliability.

Consider a scenario where our applications are generating log files and also in these log files exceptions are generated. We want these 
exceptions to be reported to the analytical engine i.e: ElasticSearch as soon as possible to track any errors.
Prior to the existing solution in the form of logstash, there were solution where applications generated log files and those log files were
transmitted over the network via FTP to the analytical engine to report the errors.

In modern day, we want all these solution to be automated and in realtime and here logstash plays important role. So logstash is also very
useful in streaming log files for real time analytics
How it will work is that we need to install logstash were our application are generating the log files. With logstash, we get the one more tool with
it which is called FileBeat which is said to be a lightweight version of logstash. Logstash is a heavy application written on java and beside that
FileBeat can do the job too of transmitting message as it is. With filebeat, we can directly ship messages to the analytical engine but that
approach won't scale much if there are many number of applications generating logs. So what we do is we install logstash on different components
in the middle so that it is scaled horizontally and filebeat can send the logs to any logstash node in a round robin fashion. Note that filebeat
does not directly sends message to logstash, infact first the log files are sent to any messaing queue like kafka or rabbitmq because they act like
a good buffer and can scale horizontally too. Logstash can pull those log files at their own processing pace (basically at a pace which the analytical
engine supports). The reason for including messaging queue is because logstash has only one queue and that queue cannot be scaled.




About Fluentd
..

Another data movement solution which can do what logstash can do but it has also some other capabilities that we can look into it.
The reason to use logstash is when using ELK stack, we use logstash as it supports it
If we are not using ELK stack, we can go with Fluentd.

Other capabilities that Fluentd offers are
  1) Memory Footprint
      .Fluentd is a very lighweight component compare to Logstash and Filebeat and it is not recommended to use Logstash on the same component where
	   the application is generating logs
	  .Fluentd has also a variation with the name Fluent Bit which is super lightweight component which we can utilize
	  .The memory comparison is as follows
	     Logstash (GB)
		 Flibeat (MB)
		 Fluentd (MB)
		 Fluent Bit (KB)
		 
  2) Routing
      .The logs that are coming out of Fluentd/Fluent Bit, can be tagged so that the logs can be routed to different analytical engine based on
	   tags. So routing feature is also important.
	   
  3) Docker Logging
      .Fluent can integrate with docker also. The docker runtime provides fluent driver by default. We need to configure the fluent config file to
	   tell the fluent driver where to transmit the messages to the analytical engine. There are multiple containers running on docker runtime like 
	   Apache Web Server, Mysql Server which can log on their console and those logs are handled by fluent driver. We just need to configure the fluent
	   config file and nothing else.
	   
	   
	   
Data Storage
--

About Elastic Search
..

After movement of data, the next thing is to consider about where to store that data

For storing the log data or any unstructured data there are 2 options
  1) Storing log files as it is in a file storage or in Hadoop HDFS (Discussed later)
  2) Storing in a NoSql database as it is designed to store unstructured data
  
Elasticsearch is a distributed, open-source search and analytics engine, which uses a NoSQL document-oriented approach to store and retrieve data. 
Therefore, it can be classified as a NoSQL database. Elasticsearch stores data as JSON documents, which can be queried and searched using its 
powerful search capabilities. However, it should be noted that Elasticsearch is primarily designed for search use cases and may not be suitable 
for all NoSQL use cases.

It is not that searching is not fast in a typical nosql database or rdbms. Using index the searches are faster but in elastic search it uses a special
kind of index which is called inverted index which allows for a full text search

The inverted index is as follows,
Consider we have a document entry in elastic search as,
  ID   Document
   1   Order Created
   2   Exception Occurred
   3   Order Delivered
   
The inverted index will be created by using the word count with corresponding document ids

  Term        Frequency   Documents
  Created        1            1
  Delivered      1            3
  Exception      1            2
  Order          2            1, 3
  
The index is sorted based on each term. In this way we can easily check where a particular text is located in documents. Lets say we want to check
how many times exceptions occurred so it will list all the documents where exceptions will are generated.

In Elastic Search, the data is not stored in a way it is mentioned. It is just the logical representation. The way it is stored is like a JSON
format which includes everything like,
..
  {
    _index: accounts
	_type: person
	_id: 1
	_source: {
	  name: John
	  lastname: Doe
	  designation: Software Engineer
	}
  }
  Here the actual document is the "source" part and everything else is the data that identifies the source field.
..

As in RDBMS we need to specify the database and table, here also we need to specify it for searching purposes but the names are different
The "_index" field represents the database
The "_type" field represents the table name

Now the searches can be done in different ways like,
  1) By ID
     Example => http://localhost:9200/accounts/person/1 (which looks for data in accounts database, person table with id = 1)
	 
  2) Open Search
     Example => http://localhost:9200/_search/q=John (will look for John in every field of source)
	 
  3) By Property
     Example => http://localhost:9200/_search/q=designation:Software (will look for text Software in designation property)
	 
  4) Search By Database/Table
     Example => http://localhost:9200/accounts/person/_search?q=designation:Software (will look for text Software in designation property in accounts
	 database and person table)
	 
	 
Elastic Search is highlt scalable, we can distribute the documents across different shards. Each shard will be responsible for some primary data
and some replica data. All the data and shards will be in track with master component. If any shard goes down then it is the master component's
responsibility to shift to replica node. In this way Elastic Search is also highly available

The Put and Get request goes to specific shard and from that shard the request is forwarded to that shard which is responsible for that data after
hashing.
The Search query works a bit differently because the data can be scattered around all nodes. The request comes on a single node by the client and
that node forwards the requests to all other nodes asynchronously. The aggregator node then combines all the result and returns back the
result. The latency is low due to asynchronous nature of forwarding request.

The Inverted Index Architecture
..
The index structure in elastic search is based on merge sort which makes it faster to add more records in it.
In Elastic Search, we dont delete the record, rather the previous record is disabled and new record is inserted. This update operation is slight
expensive because the index is needed to be scanned to find the previous record, disable it and then add new record

When a new record is inserted in elastic search, an inverted index is created with single record
When another record is inserted, a new inverted index is created with single record
Similarly consider 1 more record is inserted, 1 more inverted index is created with single record in it.
Now these 3 separated indexes can be merged into 1 index 
Now the process can repeat, when 3 more new records are inserted 3 more inverted index will be created
Now these 3 new inverted indexes can be merged into 1 index and then these 2 merged index can further be mreged to form 1 big index.
The process can now move on
Similarly, the white indexes are those indexes which are scanned when any search operation needs to be done (see image file)

These inverted indexes are maintained in write ahead log file and in memory. It is flushed on disk occassionally that is why they are very fast.
The purpose of elastic search is not to get back very large response.




About Hadoop HDFS
..

The reason to use Hadoop is that we can store the log files in HDFS as it is and can process very large amount of data
In Elastic Search we can store large amount of data but our resultset needs to be limited. If our resultset is large then elastic search will not
handle it effeciently. Elastic search is used for searching purposes only.
In Hadoop, we have a usecase of processing the data and not for searching purposes. The file size can large than 100MBs. FOr smaller files 
hadoop solution is not the right solution to pick

Conceptually Hadoop is just a file storage than can store files. The thing which makes it special is that it is a distributed file storage
If we want to store a large file lets say of size 1GB then we can configure a fixed block size lets say 100MB then that file will be split up into
10 parts and each part can be stored on a different node. Now if i want to process that file by finding any string in it or counting the words in a
file then I can process each node in parallel (using map-reduce) which will make my task 10 times faster.

When the client needs to add a file to hadoop hdfs, then there is a client library which helps client talk with the master node. The master node
has all the information that how many files are stored in hadoop and in how many parts the file is split up into. The master node will tell the
client about hadoop nodes information and client can then directly insert the files on hadoop nodes. If there are 3 nodes then the chunks will be
uploaded to node in round robin fashion. In this way if any node goes down, there will still be nodes that holds our data so data reliability is also
there.


When we install hadoop hdfs, map-reduce comes along with it. It is a processing algorithm that runs on top of hadoop cluster and helps in parallel
file processing
So Map Reduce has 2 phases
  1) Map Phase in which the filtering and transformation of data happens. We can apply our own logic also for filtering and transforming
  2) Reduce Phase in which it shuffles the outcome of Map phase, group and aggregates the data and write the final result on hadoop disk.
  
The way it works is,
Firstly the map phase will start. Suppose that we have 2 hadoop nodes for processing. When the file is uploaded on hadoop disk, in map phase, their
file is split up into 2 parts. If the file has text and has 6 lines it will be split up into 3 lines each and each 3 lines will go on for processing
on individual 2 nodes. Now on specific node, the splitted file will be further splitted into rows by line breaks so 3 lines means we will have 3
rows. Each row will now be processed in a way that the row is split up by space to check how many words are in it. After that a key value pair
will be generated to hold the result of what word was repeated how many times.
..
  Example Splitted file on Node 1
    => Apple Mango Plum   
       Plum Grapes
	   Pear Orange Apple
    => The Key value pair will be 
        (Apple, 1) - (Mango, 1) - (Plum, 1)
	    (Pear ,1) - (Grapes, 1)
	    (Pear, 1) - (Orange, 1) - (Apple, 1)
..
This processing will be done parallely on separate nodes with splitted file. This processing logic can be customized and we can do our own logic
too. After the processing, this key value pair will stored on the disk of specific node.

Now, the map phase has ended and reduce phase will be started. Before reduce phase is started, we will have to configure that on how many nodes
the reduce phase should operate. There can be single node, two nodes, three nodes, etc. This is to make sure we do reduce phase on less busy node
to improve performance. If we have chosen 2 nodes then the key value pair data will now be shuffled between these 2 nodes. The way it works is
the hash will be taken for the key and upon that hash value, that key value pair element will be either moved to Node 1 or Node 2. In this way
all the key value pairs will be shifted to relevant nodes. After that, the data is grouped to form the final result. If there were 3 Apples in whole
file scattered around 10 nodes in map phase, then all Apple keys will come together at Node 1 as the hash of Apple key. Now the value part is
grouped and aggregated  which makes the final result as (Apple, 3) which means Apple occurred 3 times in whole file. This aggregation and grouping
logic can be customized also according to our own logic.
The final result is now flushed to hadoop disk.

Also we can change the input output sources. Currently the input source is HDFS and ourpur source is also HDFS
We can change it to Cassandra or HBase input and map reduce can perform their task on hadoop cluster
Similarly we can output also to HBase or Cassandra.



There is also another tool with the name Apache Spark which is the evolution of Map Reduce
We can install apache spark on hadoop cluster itself and instead of running native hadoop map-reduce we will now run apache spark

Why use Apache Spark when there is already map-reduce?
There are several reasons
  .We know that in native map reduce, there are a total of 4 IO operations. Ar first in the map phase, the read is done from hadoop hdfs. Then after
   map phase the result is written down in specific nodes. Then in reduce phase the read is done from the nodes and then the final result is written down
   to hadoop hdfs. In Apache Spark, 2 IO operations are reduced in a way that when map phase is done, the result is not written down into nodes disk
   however they are still kept in memory and then reduce phase reads all data from the memory which makes Apache Spark 10 times and even 100 times faster.
  .In Apache Spark, instead of just map, we can do several other functions which are provided out of the box like group by, join, union, etc. These
   can also be done using modified map reduce in native method but in apache spark we donot have to write extra logic and code for that.
  .In map reduce we are restricted to use map followed by reduce operation. In Apache Spark we can do multiple operations in a graph like fashion like
   map -> union -> join -> group by -> etc. It prevents writing lot of code.
  .Apache Spark provides interactive shell in Python/R/Scala so that we can test out our flow of operations with small data sets
  .Apache Spark provides many in built libraries like 
    a) SQL interface where we can write queries and those queries can be converted into DAG (Directed Acyclic Graph) of operations
	b) Machine Learning libraries where we can run machine learning on top of spark
	c) Graph Processing libraries
	d) Streaming libraries where we can do stream processing like if we are getting data in kafka then spark can pull small amount of data, created
	   micro batches and then execute map reduce on that batch. So if we plan to do batch processing for huge amount of data on hadoop cluster then
	   spark is a very good option.
	   
	   
	   
	   
About Stream Processing
--
We do stream processing where we want the results in real time and we want analyisis of data on a real time basis like in a bank application we want
to detect frauds so these type of flows needs to be done in real time. We cannot wait for batch processing that is done after hours and even days.

So in a typical batch processing the flow is,
  We receive data from multiple sources and those data are stored in kafka to buffer that data
  That data is then pulled from logstash/fluentd at their own rate of processing and then transforms it
  After that the result is either stored on Elasticsearch or Hadoop HDFS.
  For hadoop hfds, we usually wait for a large amount of data to be stored first and then we apply map reduce on it so it eventually takes hours
  and even days for the analytics data to be prepared.
  For elastic search, it is more agile than hadoop but the data is not available in real time due to the indexing feature as it takes some time
  to prepare the index and make data available. Also, when we do search then we search on the whole data available on elastic search and not just
  the data that comes in few milliseconds ago. 
  
If we are interested in data that is available few milliseconds and few seconds ago in real time then probably elasticsearch is not the option to go 
for. We need to do something different
Instead of storing the data on Elastic Search or Hadoop, we take that data into a Stream Processing Engine. The Stream Processing Engine can be
logstash also if we want to process, transform the data on a single node because logstash/fluentd does not supports distributed processing (processing
on multipe nodes and then aggregating the data). For distributed processing we can use tools like Storm, Flink, Apache Spark
Flink is the latest tool to be used. The data procudes by streaming engine is again putted in kafka to buffer that data. From kafka that data can
be archived, sent via email or displayed on dashboard, etc according to its rate.

The stream processing engine is designed for low-latency data and high throughput data as data comes from multiple sources
Also there can be multiple event issues like some events are delayed, comes out of order or missing then we can apply logic to the engine to handle
such issues.